{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "unavailable_papers = []\n",
    "duplicates = []\n",
    "papers_directory = \"./Probabilistic_reasoning\"\n",
    "#initial csv\n",
    "initial_csv_file = \"./Probabilistic_reasoning.csv\"\n",
    "#csv directory - after removing unavailable papers\n",
    "csv_file_after_removed_unavailable_papers = \"./Probabilistic_reasoning_updated.csv\"\n",
    "#csv directory - after removing duplicates\n",
    "final_output_csv = \"./Probabilistic_reasoning_final.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def check_unavailable_papers(initial_csv_file, papers_directory):\n",
    "    \n",
    "\n",
    "    # Check if the papers directory exists\n",
    "    if not os.path.exists(papers_directory):\n",
    "        print(f\"The directory '{papers_directory}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Read the CSV file with the appropriate encoding\n",
    "    with open(initial_csv_file, 'r', encoding='utf-8-sig') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip the header row if it exists\n",
    "        for col in reader:\n",
    "            file_name = col[3]  # Assuming the fourth column contains file names\n",
    "            file_path = os.path.join(papers_directory, file_name)\n",
    "            if not file_path.endswith(\".pdf\"):\n",
    "                file_path += \".pdf\"\n",
    "            if \":\" in file_path:\n",
    "                file_path = file_path.replace(\":\", \"_\")\n",
    "            if not os.path.exists(file_path):\n",
    "                unavailable_papers.append(file_name)\n",
    "\n",
    "    if unavailable_papers:\n",
    "        print(\"Unavailable papers:\")\n",
    "        for paper in unavailable_papers:\n",
    "            print(paper)\n",
    "    else:\n",
    "        print(\"All papers are available.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_unavailable_papers(initial_csv_file, csv_file_after_removed_unavailable_papers):\n",
    "\n",
    "    if not unavailable_papers:\n",
    "        print(\"No unavailable papers to remove.\")\n",
    "        return\n",
    "\n",
    "    # Read the original CSV file and write a new one excluding unavailable papers\n",
    "    with open(initial_csv_file, 'r', encoding='utf-8-sig') as infile, open(csv_file_after_removed_unavailable_papers, 'w', newline='', encoding='utf-8-sig') as outfile:\n",
    "        reader = csv.reader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "\n",
    "        header = next(reader)  # Read the header\n",
    "        writer.writerow(header)  # Write the header to the new file\n",
    "\n",
    "        for row in reader:\n",
    "            file_name = row[3]  # Assuming the fourth column contains file names\n",
    "            if file_name not in unavailable_papers:\n",
    "                writer.writerow(row)\n",
    "\n",
    "    print(f\"Unavailable papers removed. Updated CSV saved as '{csv_file_after_removed_unavailable_papers}'.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def find_duplicate_records(csv_file):\n",
    "    records = defaultdict(int)\n",
    "\n",
    "    # Read the CSV file with the appropriate encoding\n",
    "    with open(csv_file, 'r', encoding='utf-8-sig') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)  # Skip the header row\n",
    "        for row in reader:\n",
    "            record = tuple(row)  # Convert the row to a tuple to make it hashable\n",
    "            records[record] += 1\n",
    "            if records[record] == 2:  # Only add to duplicates list the first time a duplicate is found\n",
    "                duplicates.append(record)\n",
    "\n",
    "    if duplicates:\n",
    "        print(\"Duplicate records found:\")\n",
    "        for duplicate in duplicates:\n",
    "            print(duplicate)\n",
    "    else:\n",
    "        print(\"No duplicate records found.\")\n",
    "    return duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_records(csv_file_after_removed_duplicates, final_output_csv):\n",
    "    records = set()\n",
    "    duplicates_removed = 0\n",
    "\n",
    "    # Read the original CSV file and write a new one excluding duplicate records\n",
    "    with open(csv_file_after_removed_duplicates, 'r', encoding='utf-8-sig') as infile, open(final_output_csv, 'w', newline='', encoding='utf-8-sig') as outfile:\n",
    "        reader = csv.reader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "\n",
    "        header = next(reader)  # Read the header\n",
    "        writer.writerow(header)  # Write the header to the new file\n",
    "\n",
    "        for row in reader:\n",
    "            record = tuple(row)  # Convert the row to a tuple to make it hashable\n",
    "            if record not in records:\n",
    "                writer.writerow(row)\n",
    "                records.add(record)\n",
    "            else:\n",
    "                duplicates_removed += 1\n",
    "\n",
    "    print(f\"Duplicate records removed: {duplicates_removed}. Updated CSV saved as '{final_output_csv}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unavailable papers:\n",
      "General Robust Subband Adaptive Filtering_ Algorithms and Applications.pdf\n",
      "Improved Speech Enhancement Considering Speech PSD Uncertainty.pdf\n",
      "Fuzzy-Based Multiobjective Multifactor Dimensionality Reduction for Epistasis Analysis.pdf\n",
      "Decidability and complexity in weakening and contraction hypersequent substructural logics.pdf\n",
      "Perspective multi-player games.pdf\n",
      "Fuzzy and Rough Set Theory Based Computational Framework for Mining Genetic Interaction Triplets From Gene Expression Profiles for Lung Adenocarcinoma.pdf\n",
      " convergence\n",
      "Robust Voice Feature Selection Using Interval Type-2 Fuzzy AHP for Automated Diagnosis of Parkinson&#x0027;s Disease.pdf\n"
     ]
    }
   ],
   "source": [
    "#check unavilable papers\n",
    "check_unavailable_papers(initial_csv_file, papers_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unavailable papers removed. Updated CSV saved as './Probabilistic_reasoning_updated.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "remove_unavailable_papers(initial_csv_file, csv_file_after_removed_unavailable_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate records found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find duplicate records\n",
    "find_duplicate_records(csv_file_after_removed_unavailable_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate records removed: 0. Updated CSV saved as './Probabilistic_reasoning_final.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate records\n",
    "remove_duplicate_records(csv_file_after_removed_unavailable_papers, final_output_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
