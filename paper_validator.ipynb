{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "unavailable_papers = []\n",
    "duplicates = []\n",
    "papers_directory = \"papers/Shape inference\"\n",
    "#initial csv\n",
    "initial_csv_file = \"papers/Shape inference.csv\"\n",
    "#csv directory - after removing unavailable papers\n",
    "csv_file_after_removed_unavailable_papers = \"temp.csv\"\n",
    "#csv directory - after removing duplicates\n",
    "final_output_csv = \"Shape inference_final.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def check_unavailable_papers(initial_csv_file, papers_directory):\n",
    "    \n",
    "\n",
    "    # Check if the papers directory exists\n",
    "    if not os.path.exists(papers_directory):\n",
    "        print(f\"The directory '{papers_directory}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Read the CSV file with the appropriate encoding\n",
    "    with open(initial_csv_file, 'r', encoding='utf-8-sig',errors='replace') as file:\n",
    "        reader = csv.reader(file)\n",
    "        # next(reader)  # Skip the header row if it exists\n",
    "        for col in reader:\n",
    "            file_name = col[3]  # Assuming the fourth column contains file names\n",
    "            file_path = os.path.join(papers_directory, file_name)\n",
    "            if not file_path.endswith(\".pdf\"):\n",
    "                file_path += \".pdf\"\n",
    "            if \":\" in file_path:\n",
    "                file_path = file_path.replace(\":\", \"_\")\n",
    "            if not os.path.exists(file_path):\n",
    "                unavailable_papers.append(file_name)\n",
    "\n",
    "    if unavailable_papers:\n",
    "        print(\"Unavailable papers:\")\n",
    "        for paper in unavailable_papers:\n",
    "            print(paper)\n",
    "    else:\n",
    "        print(\"All papers are available.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_unavailable_papers(initial_csv_file, csv_file_after_removed_unavailable_papers):\n",
    "\n",
    "    if not unavailable_papers:\n",
    "        print(\"No unavailable papers to remove.\")\n",
    "        return\n",
    "\n",
    "    # Read the original CSV file and write a new one excluding unavailable papers\n",
    "    with open(initial_csv_file, 'r', encoding='utf-8-sig',errors='replace') as infile, open(csv_file_after_removed_unavailable_papers, 'w', newline='', encoding='utf-8-sig',errors='replace') as outfile:\n",
    "        reader = csv.reader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "\n",
    "        # header = next(reader)  # Read the header\n",
    "        # writer.writerow(header)  # Write the header to the new file\n",
    "\n",
    "        for row in reader:\n",
    "            file_name = row[3]  # Assuming the fourth column contains file names\n",
    "            if file_name not in unavailable_papers:\n",
    "                writer.writerow(row)\n",
    "\n",
    "    print(f\"Unavailable papers removed. Updated CSV saved as '{csv_file_after_removed_unavailable_papers}'.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def find_duplicate_records(csv_file):\n",
    "    records = defaultdict(int)\n",
    "\n",
    "    # Read the CSV file with the appropriate encoding\n",
    "    with open(csv_file, 'r', encoding='utf-8-sig',errors='replace') as file:\n",
    "        reader = csv.reader(file)\n",
    "        # header = next(reader)  # Skip the header row\n",
    "        for row in reader:\n",
    "            record = tuple(row)  # Convert the row to a tuple to make it hashable\n",
    "            records[record] += 1\n",
    "            if records[record] == 2:  # Only add to duplicates list the first time a duplicate is found\n",
    "                duplicates.append(record)\n",
    "\n",
    "    if duplicates:\n",
    "        print(\"Duplicate records found:\")\n",
    "        for duplicate in duplicates:\n",
    "            print(duplicate)\n",
    "    else:\n",
    "        print(\"No duplicate records found.\")\n",
    "    return duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_records(csv_file_after_removed_duplicates, final_output_csv):\n",
    "    records = set()\n",
    "    duplicates_removed = 0\n",
    "\n",
    "    # Read the original CSV file and write a new one excluding duplicate records\n",
    "    with open(csv_file_after_removed_duplicates, 'r', encoding='utf-8-sig',errors='replace') as infile, open(final_output_csv, 'w', newline='', encoding='utf-8-sig',errors='replace') as outfile:\n",
    "        reader = csv.reader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "\n",
    "        # header = next(reader)  # Read the header\n",
    "        # writer.writerow(header)  # Write the header to the new file\n",
    "\n",
    "        for row in reader:\n",
    "            record = tuple(row)  # Convert the row to a tuple to make it hashable\n",
    "            if record not in records:\n",
    "                writer.writerow(row)\n",
    "                records.add(record)\n",
    "            else:\n",
    "                duplicates_removed += 1\n",
    "\n",
    "    print(f\"Duplicate records removed: {duplicates_removed}. Updated CSV saved as '{final_output_csv}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unavailable papers:\n",
      "Region Growing for Segmenting Green Microalgae Images.pdf\n",
      "Towards Letter Shape Prior and Paleographic Tables Estimation in Hebrew First Temple Period Ostraca.pdf\n",
      "Automatic Adjustment of Stereoscopic Content for Long-Range Projections in Outdoor Areas.pdf\n",
      "Effect of Inhibitory Window on Event-Based Hough Transform for Multiple Lines Detection.pdf\n",
      "Accurate geo-localization of low-altitude overhead images from 3D point clouds.pdf\n",
      "A Phase Unwrapping Method for Large Scale Interferometric Phase Images.pdf\n",
      "License plate image patch filtering using HOG descriptor and bio-inspired optimization.pdf\n",
      "From Reassembly to Object Completion_ A Complete Systems Pipeline.pdf\n",
      "Muscle Tissue Labeling of Human Lower Limb in Multi-Channel mDixon MR Imaging_ Concepts and Applications.pdf\n",
      "On the (soccer) ball.pdf\n",
      "Real-time head pose estimation for driver assistance system using low-cost on-board computer.pdf\n",
      "Projective-AR system for customizing the appearance and taste of food.pdf\n",
      "Eyes wide open_ eyelid location and eye aperture estimation for pervasive eye tracking in real-world scenarios.pdf\n",
      "Extraction of Blebs in Human Embryonic Stem Cell Videos.pdf\n",
      "Multi-view 3D reconstruction_ a scene-based, visual hull guided, multi-stereovision framework.pdf\n",
      "Combining photometric normals and multi-view stereo for 3D reconstruction.pdf\n",
      "Phasor Imaging_ A Generalization of Correlation-Based Time-of-Flight Imaging.pdf\n",
      "Shape Representation and Classification Based on Discrete Cosine Transformation and IDSC.pdf\n",
      "On using the D2 descriptor in search for the best view.pdf\n",
      "An evolutionary approach for image segmentation.pdf\n",
      "Object vs. Objectless Motion.pdf\n",
      "3D Digital Microscopy for Characterizing Punchworks on Medieval Panel Paintings.pdf\n",
      "A three-way investigation of a game-CAPTCHA_ automated attacks, relay attacks and usability.pdf\n",
      "A Discrete Approach for Pairwise Matching of Archaeological Fragments.pdf\n",
      "Creating custom fitted point of goggles through facial metrics.pdf\n",
      "A supervised learning approach for fast object recognition from RGB-D data.pdf\n",
      "Automatic 3D object reconstruction from a single image.pdf\n",
      "Adaptive planar and rotational image stitching for mobile devices.pdf\n",
      "Coupled structure-from-motion and 3D symmetry detection for urban facades.pdf\n",
      "Multi-modal descriptors for multi-class hand pose recognition in human computer interaction systems.pdf\n",
      "Salience-based feature preserving resizing for 3D models.pdf\n",
      "Iterative cage-based registration from multi-view silhouettes.pdf\n",
      "Image-based food volume estimation.pdf\n",
      "Attribit_ content creation with semantic attributes.pdf\n",
      "Segmentation of histological images using a metaheuristic-based level set approach.pdf\n",
      "Adding obstacle avoidance to a robotic platform for human robot interaction.pdf\n",
      "Developing visual competencies for socially assistive robots_ the HOBBIT approach.pdf\n",
      "A Recent Survey on Colon Cancer Detection Techniques.pdf\n",
      "A parts-based approach for automatic 3D shape categorization using belief functions.pdf\n",
      "Hand detection and feature extraction for static Thai Sign Language recognition.pdf\n",
      "Reconstructing partially visible models using stereo vision, structured light, and the g2o framework.pdf\n",
      "Improved segmentation for footprint recognition of small mammals.pdf\n",
      "A structured template based 3D face recognition approach.pdf\n",
      "Intrinsic parameters calibration for multi-beam LiDAR using the Levenberg-Marquardt algorithm.pdf\n",
      "XKin -_ eXtendable hand pose and gesture recognition library for kinect.pdf\n",
      "Detecting text in the real world.pdf\n",
      "Real-time pedestrian learning-tracking with information fusion.pdf\n",
      "Image retrieval system based on feature extraction and relevance feedback.pdf\n",
      "Evolving a conspicuous point detector based on an artificial dorsal stream_ SLAM system.pdf\n",
      "Vision based indoor scene localization via smart phone.pdf\n",
      "Color CENTRIST_ a color descriptor for scene categorization.pdf\n",
      "GPU-based real-time spatio-temporal reconstruction studio.pdf\n",
      "3D perceptual shape feature-based body parts classification and pose estimation.pdf\n",
      "Estimation and utilization of articulations in recovering non-rigid structure from motion using motion subspaces.pdf\n",
      "Local features for partial shape matching and retrieval.pdf\n",
      "Robust hand gesture recognition with kinect sensor.pdf\n",
      "Robust hand gesture recognition based on finger-earth mover_s distance with a commodity depth camera.pdf\n",
      "Categorizing CAPTCHA.pdf\n",
      "3D model retrieval using accurate pose estimation and view-based similarity.pdf\n",
      "A contour based scheme for representing arbitrary shapes in digital images.pdf\n",
      "Image-based dress up system.pdf\n"
     ]
    }
   ],
   "source": [
    "#check unavilable papers\n",
    "check_unavailable_papers(initial_csv_file, papers_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unavailable papers removed. Updated CSV saved as 'temp.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "remove_unavailable_papers(initial_csv_file, csv_file_after_removed_unavailable_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate records found.\n"
     ]
    }
   ],
   "source": [
    "# Find duplicate records\n",
    "if os.path.exists(csv_file_after_removed_unavailable_papers):\n",
    "  find_duplicate_records(csv_file_after_removed_unavailable_papers)\n",
    "else:\n",
    "  find_duplicate_records(initial_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate records removed: 0. Updated CSV saved as 'Shape inference_final.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate records\n",
    "if os.path.exists(csv_file_after_removed_unavailable_papers):\n",
    "    remove_duplicate_records(csv_file_after_removed_unavailable_papers, final_output_csv)\n",
    "else:\n",
    "    remove_duplicate_records(initial_csv_file,final_output_csv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
